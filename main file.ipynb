{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-09T10:01:39.165526Z","iopub.execute_input":"2025-02-09T10:01:39.165862Z","iopub.status.idle":"2025-02-09T10:01:43.337014Z","shell.execute_reply.started":"2025-02-09T10:01:39.165835Z","shell.execute_reply":"2025-02-09T10:01:43.336150Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom torch.utils.data import Dataset, DataLoader, IterableDataset\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WebsightIterableDataset(IterableDataset):\n    def __init__(self, processor, tokenizer, max_length=512, validation=False, val_ratio=0.1):\n        self.dataset = load_dataset(\n            \"HuggingFaceM4/WebSight\",\n            \"v0.1\",\n            split=\"train\",\n            streaming=True\n        )\n        self.processor = processor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.validation = validation\n        self.val_ratio = val_ratio\n\n    def __iter__(self):\n        iterator = iter(self.dataset)\n        while True:\n            try:\n                item = next(iterator)\n                if 'screenshot' not in item or 'code' not in item:\n                    continue\n\n                is_val = hash(item['code']) % 10 < (self.val_ratio * 10)\n                if is_val != self.validation:\n                    continue\n\n                # Process image (resize and normalize)\n                image = self.processor(item['screenshot'])\n\n                # Tokenize HTML\n                html_tokens = self.tokenizer(\n                    item['code'],\n                    max_length=self.max_length,\n                    padding=\"max_length\",\n                    truncation=True,\n                    return_tensors=\"pt\"\n                )\n\n                yield {\n                    'image': image,\n                    'html_input_ids': html_tokens.input_ids[0],\n                    'html_attention_mask': html_tokens.attention_mask[0],\n                    'raw_html': item['code']\n                }\n            except StopIteration:\n                break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageToHTMLGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #  MobileNetV3 for feature extraction\n        mobilenet = models.mobilenet_v3_small(pretrained=True)\n        self.image_encoder = nn.Sequential(*list(mobilenet.children())[:-1])  # Remove classifier\n        \n        self.projection = nn.Linear(576, 768)  # Match BART hidden dim\n\n        #  DistilBART for text generation\n        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n\n    def forward(self, image, html_input_ids=None, html_attention_mask=None):\n        with torch.no_grad():\n            image_features = self.image_encoder(image).squeeze(-1).squeeze(-1)  # Extract feature map\n        \n        projected_features = self.projection(image_features).unsqueeze(1)  # Align with BART's hidden dim\n\n        if html_input_ids is not None:\n            outputs = self.bart(\n                input_ids=html_input_ids,\n                attention_mask=html_attention_mask,\n                encoder_outputs=(projected_features,),\n                labels=html_input_ids\n            )\n            return outputs\n        else:\n            generated = self.bart.generate(\n                encoder_outputs=(projected_features,),\n                max_length=256,\n                num_beams=2,\n                early_stopping=True\n            )\n            return generated\n\ndef create_data_loaders(processor, tokenizer, batch_size=8):\n    train_dataset = WebsightIterableDataset(processor, tokenizer, validation=False)\n    val_dataset = WebsightIterableDataset(processor, tokenizer, validation=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, prefetch_factor=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2, prefetch_factor=2)\n\n    return train_loader, val_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n    best_val_loss = float('inf')\n    steps_per_epoch = 250\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        train_steps = 0\n\n        train_pbar = tqdm(enumerate(train_loader), total=steps_per_epoch, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n        \n        for step, batch in train_pbar:\n            if step >= steps_per_epoch:\n                break\n                \n            try:\n                image = batch['image'].to(device, non_blocking=True)\n                html_input_ids = batch['html_input_ids'].to(device, non_blocking=True)\n                html_attention_mask = batch['html_attention_mask'].to(device, non_blocking=True)\n\n                outputs = model(\n                    image=image,\n                    html_input_ids=html_input_ids,\n                    html_attention_mask=html_attention_mask\n                )\n\n                loss = outputs.loss\n                train_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n\n                train_steps += 1\n                train_pbar.set_postfix({'loss': train_loss / (step + 1)})\n            except Exception as e:\n                print(f\"Error in training batch: {e}\")\n                continue\n\n        avg_train_loss = train_loss / train_steps\n\n        model.eval()\n        val_loss = 0\n        val_steps = 0\n\n        val_pbar = tqdm(enumerate(val_loader), total=steps_per_epoch // 5, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n        \n        with torch.no_grad():\n            for step, batch in val_pbar:\n                if step >= steps_per_epoch // 5:\n                    break\n                    \n                try:\n                    image = batch['image'].to(device)\n                    html_input_ids = batch['html_input_ids'].to(device)\n                    html_attention_mask = batch['html_attention_mask'].to(device)\n\n                    outputs = model(\n                        image=image,\n                        html_input_ids=html_input_ids,\n                        html_attention_mask=html_attention_mask\n                    )\n\n                    val_loss += outputs.loss.item()\n                    val_steps += 1\n                    val_pbar.set_postfix({'loss': val_loss / (step + 1)})\n                    \n                except Exception as e:\n                    print(f\"Error in validation batch: {e}\")\n                    continue\n\n        avg_val_loss = val_loss / val_steps\n        scheduler.step(avg_val_loss)\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_val_loss': best_val_loss,\n            }, \"best_model.pth\")\n        \n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'Training Loss: {avg_train_loss:.4f}')\n        print(f'Validation Loss: {avg_val_loss:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processor = models.mobilenet_v3_small(pretrained=True).features  # Image feature extractor\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n    \ntrain_loader, val_loader = create_data_loaders(processor, tokenizer)\n \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ImageToHTMLGenerator().to(device)\nprint('\\n', device)\n\ntrain_model(model, train_loader, val_loader, num_epochs=10, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T11:14:59.770917Z","iopub.execute_input":"2025-02-09T11:14:59.771282Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n100%|██████████| 9.83M/9.83M [00:00<00:00, 97.7MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9b73c8de60d4e198b7d846e427254b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30cc220817d2478ba8924f76ebf6fbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64128c893cca4665bdd89a9bccb57018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c22e981780490fbb2b00979f0d65e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f5eb4654d50482784e6ff33b04f313d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/738 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26241aeef5dd4d189edc379313e91f8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/71 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9a4b8422a24939a6bd21c1680cf408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/738 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a2bf421f5c447eba686801155589fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/71 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffffd8e13506436caf58ee7a3e13f432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0491d74ae3f34e25be8b6d3498903d1f"}},"metadata":{}},{"name":"stdout","text":"\n cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 [Train]:   0%|          | 0/250 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}